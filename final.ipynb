{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ddb898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "from resnet import resnet\n",
    "\n",
    "from feather import Pruner\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6566ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(size=32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                         std=[0.2023, 0.1994, 0.201]),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                         std=[0.2023, 0.1994, 0.201]),\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12d3416",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'sparsity_type': \"feather\", # nm / entropy / feather / spartan / ses / base (no sparsity)\n",
    "    'epochs': 100,\n",
    "    'lr': 0.1,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4,\n",
    "    'batch_size': len(train_loader),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e307813",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity_type = hyperparameters['sparsity_type']\n",
    "output_base_path = \"/home/sg666/Class/ECE661/outputs\"\n",
    "sparsity_folder_path = os.path.join(output_base_path, sparsity_type)\n",
    "\n",
    "hyperparameter_str = f\"epochs_{hyperparameters['epochs']}_lr_{hyperparameters['lr']}_momentum_{hyperparameters['momentum']}_wd_{hyperparameters['weight_decay']}_batch_{hyperparameters['batch_size']}\"\n",
    "output_folder = os.path.join(sparsity_folder_path, hyperparameter_str)\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "hyperparameter_file = os.path.join(output_folder, 'hyperparameters.txt')\n",
    "with open(hyperparameter_file, 'w') as f:\n",
    "    for key, value in hyperparameters.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, epoch, log_file):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\", ncols=100)\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "        if sparsity_type == 'feather': pruner.update_thresh()\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "        pbar.set_postfix(loss=running_loss/(batch_idx+1), accuracy=100.0 * correct / total)\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100.0 * correct / total\n",
    "    log_file.write(f'Epoch [{epoch+1}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\\n')\n",
    "    if sparsity_type == 'feather': pruner.update_thresh(end_of_batch=True)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def test(model, test_loader, criterion, log_file):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(test_loader, desc=\"Testing\", ncols=100)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in pbar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "            pbar.set_postfix(loss=test_loss/(total + inputs.size(0)), accuracy=100.0 * correct / total)\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    accuracy = 100.0 * correct / total\n",
    "    log_file.write(f'Test Loss: {avg_test_loss:.4f}, Accuracy: {accuracy:.2f}%\\n')\n",
    "\n",
    "    return avg_test_loss, accuracy\n",
    "\n",
    "\n",
    "resnet20_model = resnet()\n",
    "resnet20_model.to(device)\n",
    "\n",
    "# TODO: decide how robust our sparsity implementations will be. I suggest following what the feather paper did and using a pruner class.\n",
    "if sparsity_type == 'base':\n",
    "    pruner = resnet20_model\n",
    "elif sparsity_type == 'feather':\n",
    "    # Initialize the pruner with the model, device, and desired sparsity\n",
    "    pruner = Pruner(resnet20_model, device, final_rate=0.95, nbatches=hyperparameters['batch_size'], epochs=hyperparameters['epochs'])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet20_model.parameters(), lr=hyperparameters['lr'], \n",
    "                      momentum=hyperparameters['momentum'], weight_decay=hyperparameters['weight_decay'])\n",
    "\n",
    "# TODO: implement a learning rate scheduler\n",
    "\n",
    "log_file_path = os.path.join(output_folder, 'training_log.txt')\n",
    "with open(log_file_path, 'w') as log_file:\n",
    "    log_file.write(f\"Training started at {datetime.now()}\\n\")\n",
    "\n",
    "    # I don't think we should be deliniating between nm and other sparsity types in the training loop\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(hyperparameters['epochs']):\n",
    "        train_loss, train_accuracy = train(resnet20_model, train_loader, criterion, optimizer, epoch, log_file)\n",
    "        test_loss, test_accuracy = test(resnet20_model, test_loader, criterion, log_file)\n",
    "\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "            model_checkpoint_path = os.path.join(output_folder, f\"model_best.pth\")\n",
    "            torch.save(resnet20_model.state_dict(), model_checkpoint_path)\n",
    "            print(f\"Saved best model at epoch {epoch+1} with accuracy: {best_accuracy:.2f}%\")\n",
    "    \n",
    "    # best_accuracy = 0.0\n",
    "\n",
    "    # if sparsity_type == 'nm':\n",
    "    #     for epoch in range(hyperparameters['epochs']):\n",
    "    #         train_loss, train_accuracy = train(resnet20_model, train_loader, criterion, optimizer, epoch, log_file)\n",
    "    #         test_loss, test_accuracy = test(resnet20_model, test_loader, criterion, log_file)\n",
    "\n",
    "    #         if test_accuracy > best_accuracy:\n",
    "    #             best_accuracy = test_accuracy\n",
    "    #             model_checkpoint_path = os.path.join(output_folder, f\"model_best.pth\")\n",
    "    #             torch.save(resnet20_model.state_dict(), model_checkpoint_path)\n",
    "    #             print(f\"Saved best model at epoch {epoch+1} with accuracy: {best_accuracy:.2f}%\")\n",
    "\n",
    "    # else:\n",
    "    #     if sparsity_type == \"entropy\":\n",
    "    #         pass  # Apply entropy sparsity\n",
    "    #     elif sparsity_type == \"feather\":\n",
    "    #         pass # Apply feather sparsity\n",
    "    #     elif sparsity_type == \"spartan\":\n",
    "    #         pass  # Apply spartan sparsity\n",
    "    #     elif sparsity_type == 'ses':\n",
    "    #         pass  # Apply SES sparsity\n",
    "    #     elif sparsity_type == \"base\":\n",
    "    #         pass  # Apply no sparsity\n",
    "\n",
    "    #     for epoch in range(hyperparameters['epochs']):\n",
    "    #         train_loss, train_accuracy = train(resnet20_model, train_loader, criterion, optimizer, epoch, log_file)\n",
    "    #         test_loss, test_accuracy = test(resnet20_model, test_loader, criterion, log_file)\n",
    "\n",
    "    #         if test_accuracy > best_accuracy:\n",
    "    #             best_accuracy = test_accuracy\n",
    "    #             model_checkpoint_path = os.path.join(output_folder, f\"model_best.pth\")\n",
    "    #             torch.save(resnet20_model.state_dict(), model_checkpoint_path)\n",
    "    #             print(f\"Saved best model at epoch {epoch+1} with accuracy: {best_accuracy:.2f}%\")\n",
    "\n",
    "    log_file.write(f\"Training completed at {datetime.now()}\\n\")\n",
    "    log_file.write(f\"Best model accuracy: {best_accuracy:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bf5edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sparsity_type == \"base\":\n",
    "    print(\"Base model training completed.\")\n",
    "elif sparsity_type == \"feather\":\n",
    "    print(\"Feather sparsity training completed.\")\n",
    "    pr = pruner.print_sparsity()\n",
    "    print(f\"prune rate : {pr}\" )\n",
    "    pruner.desparsify()\n",
    "    torch.save(resnet20_model.state_dict(), model_checkpoint_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
